---
output: html_document
---
<!-- Configuración de celdas e input -->
```{r setup, include=FALSE}
knitr::opts_chunk$set(
  root.dir = "/home/andy/Git/ia-model_eval/",
  results = 'hold'
)
library(caret)
library(klaR)
library(dplyr)
library(knitr)
library(kableExtra)
library(AUC)
library(kernlab)
library(e1071)
library(pROC)
library(ROCR)
library(plyr)
library(C50)
```

<!-- Custom Header -->
---
<div style="margin-top: 50px;"></div>

<h1 style="text-align: center; font-size: 36px; font-family: Arial, sans-serif; color: #2c3e50;">
    <strong>Entrega B: R - Evaluación de Modelos</strong>
</h1>
<p style="text-align: center; font-size: 18px; font-family: Arial;">
    <strong>Andrés García-Serra Romero</strong> <br>
    <strong>Ciencia de Datos</strong>
</p>

<!-- Introduccion -->
---
<div style="margin-top: 20px;"></div>
<h1 style="font-size: 24px; font-family: Arial, sans-serif; color: #2c3e50;">
    <strong>Introducción</strong>
</h1>

En este trabajo aplicaremos los conocimientos aprendidos en evaluación de diferentes tipos de clasificadores de datos en la tarea de predicción de si un indivíduo ha ganado o no una partida de tres en raya, utilizando como datos de input la distribución del tablero de la partida. Para realizar la práctica hemos utilizado el software RStudio, creando un proyecto del tipo Markdown notebook. Utilizando el paquete *caret* de *R* crearemos cuatro modelos ...

<!-- 1. Importando datos -->
---
<h1 style="font-size: 24px; font-family: Arial, sans-serif; color: #2c3e50;">
    <strong>1.Importar Datos</strong>
</h1>

En primer lugar importaremos el fichero *tic-tac-toe.txt* con los datos en un dataframe, del que cambiaremos los nombres de las columnas para comprender mejor los datos, teniendo en cuenta de que se trata de un tablero 3x3 en el que las posiciones van desde arriba a la izquierda (*pos1*) hasta debajo a la derecha (*pos9*). Llamaremos *win* al último elemento de cada fila, que recoge la victoria o derrota del jugador "x".

```{r}
tictac <- data.frame(read.csv("tic-tac-toe.txt"))
header <- c("pos1","pos2","pos3","pos4","pos5","pos6","pos7","pos8","pos9","win")
colnames(tictac) <- header
head(tictac)
```

Como primer paso analizamos los datos en busca de valores vacíos, NaNs o valores que no sean los deseados. Estos son: "x", "o" o "b" entre los componentes de posición y "positive" o "negative" en el último componente de cada fila.

```{r}
any(is.na(tictac))
values <- c("x", "o", "b","positive", "negative")
any(!as.matrix(tictac[, 1:9]) %in% values[1:3])
any(!tictac[, 10] %in% values[4:5])

```

<!-- 2. Data Splitting -->
---
<h1 style="font-size: 24px; font-family: Arial, sans-serif; color: #2c3e50;">
    <strong>2. Data Splitting</strong>
</h1>

Dividimos el dataset en 70 training y 30 test. Comprobamos que efectivamente la proporción es 7/3.

```{r}
set.seed(100)
train_pos <- createDataPartition(tictac$win, p = .7, 
                                  list = FALSE, 
                                  times = 1)
tictac_train <- tictac[ train_pos,]
tictac_test  <- tictac[-train_pos,]
```

Comprobamos que la división de los datasets sigue la proporción 7/3 y también que la proporción de clases de ambos datasets es igual que la del oiginal.

```{r}
# Ratio entre el Set de Entrenamiento y el de Evaluación respecto a 7/3
test_train_ratio <- data.frame(
  Name = c("Sets Size Ratio to 7/3"),
  value = dim(tictac_train)[1]/dim(tictac_test)[1] / (7/3)
)
kable(test_train_ratio, format = "html") %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover"))

# Ratios de Positivos/Negativos para cada set
sets <- data.frame(
  Set = c("Pos/Neg Ratio"),
  Full_Dataset = c(sum(tictac[,10]=="positive")/sum(tictac[,10]=="negative")),
  Training_Set = c(sum(tictac_train[,10]=="positive")/sum(tictac_train[,10]=="negative")),
  Validation_Set = c(sum(tictac_test[,10]=="positive")/sum(tictac_test[,10]=="negative"))
)
kable(sets, col.names = c("", "Full Dataset", "Training Set", "Validation Set"), format = "html") %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover"))

```

<!-- 3. Generación de modelos -->
---
<h1 style="font-size: 24px; font-family: Arial, sans-serif; color: #2c3e50;">
    <strong>3. Generación de Modelos</strong>
</h1>

En primer lugar llevaremos a cabo el entrenamiento del modelo utilizando el set de entrenamiento ya creado. Para el trabajo de comparativa usaremos los modelos: Naive Bayes, Decision Tree, Neural Network, Nearest Neighbour y SVM (kernel lineal). En esta pieza de código haremos el entrenamiento y sacaremos las métricas de Accuracy y Kappa en el entrenamiento para cada uno de ellos, recogidos en una tabla.

```{r, results='hide'}
set.seed(100)

fitControl <- trainControl(
  method = "cv",              # cross-validation
  number = 10,                # 10 folds
  classProbs = TRUE           # class probabilities for later use
)

set.seed(100)
models <- list(
  nb_train <- train(win ~ ., data = tictac_train, method = "nb", trControl = fitControl),
  dt_train <- train(win ~ ., data = tictac_train, method = "C5.0", trControl = fitControl),
  nn_train <- train(win ~ ., data = tictac_train, method = "nnet", trControl = fitControl),
  knn_train <- train(win ~ ., data = tictac_train, method = "knn", trControl = fitControl),
  SVM_train <- train(win ~ ., data = tictac_train, method = "svmLinear2", trControl = fitControl)
)

results <- bind_rows(lapply(models, function(model) {
  data.frame(
    Accuracy = max(model$results$Accuracy),
    Kappa = max(model$results$Kappa)
  )
}))

Models <- c('Naive Bayes','Decision Tree','Neural Network','Nearest Neighbour','SVM (linear kernel)')
results <- cbind(Models, results)

```


```{r}
# Mostrar tabla con estilo en HTML
kable(results, format = "html") %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover"))
```

Podemos ver como, para este tipo de problema y de datos de entrenamiento, los modelos que mejor funcionan, o al menos siguiendo el criterio de las métricas seleccionadas, son el de Red Neuronal y SVM con kernel lineal.

<h1 style="font-size: 20px; font-family: Arial, sans-serif; color: #2c3e50;">
    <strong>4. Evaluación de Modelos</strong>
</h1>

Podemos ahora tomar los modelos y evaluarlos usando nuestro set de entrenamiento. Usando la herramienta *postResample* aplicada al modelo de red neuronal podemos extraer la matriz de confusión de esta evaluación, así como distintas métricas de evaluación, como Accuracy y Kappa entre otras.

```{r}
nn_pred <- predict(nn_train, tictac_test, type = "prob")
roc_curve <- roc(nn_pred, factor(tictac_test$win))
auc_value <- auc(roc_curve)
print(auc_value)
```
Repitiendo esto para el resto de modelos, podemos extraer una tabla de accuracy y kappa de cada modelo aplicado a los datos de evaluación.

```{r}
set.seed(100)
nb_pred <- predict(nb_train,tictac_test, type = "prob")
dt_pred <- predict(dt_train,tictac_test, type = "prob")
nn_pred <- predict(nn_train,tictac_test, type = "prob")
knn_pred <- predict(knn_train,tictac_test, type = "prob")
SVM_pred <- predict(SVM_train,tictac_test, type = "prob")

set.seed(100)
eval <- list(
  nb_eval <- postResample(nb_pred,factor(tictac_test$win)),
  dt_eval <- postResample(dt_pred,factor(tictac_test$win)),
  nn_eval <- postResample(nn_pred,factor(tictac_test$win)),
  knn_eval <- postResample(knn_pred,factor(tictac_test$win)),
  SVM_eval <- postResample(SVM_pred,factor(tictac_test$win))
  )

eval_results <- bind_rows(lapply(eval, function(model) {
  data.frame(
    Accuracy = model[[1]],  # Mejor accuracy
    Kappa = model[[2]]      # Mejor kappa
  )
}))

eval_results <- cbind(Models, eval_results)

AUC <- c(auc(roc(nb_pred,factor(tictac_test$win))),
         auc(roc(dt_pred,factor(tictac_test$win))),
         auc(roc(nn_pred,factor(tictac_test$win))),
         auc(roc(knn_pred,factor(tictac_test$win))),
         auc(roc(SVM_pred,factor(tictac_test$win)))
         )


eval_results <- cbind(eval_results,AUC)

kable(eval_results, format = "html") %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover"))

```

```{r}
  set.seed(100)
  positives <- ifelse(tictac_test$win == "positive", 1, 0)
  nb_prob <- predict(nb_train, tictac_test, type = "prob")
  dt_prob <- predict(dt_train, tictac_test, type = "prob")
  nn_prob <- predict(nn_train, tictac_test, type = "prob")
  knn_prob <- predict(knn_train, tictac_test, type = "prob")
  #SVM_prob <- predict(SVM_train, tictac_test, type = "prob")
  
  nb_pred <- prediction(nb_prob[,2], tictac_test$win)
  dt_pred <- prediction(dt_prob[,2], tictac_test$win)
  nn_pred <- prediction(nn_prob[,2], tictac_test$win)
  knn_pred <- prediction(knn_prob[,2], tictac_test$win)
  #SVM_pred <- prediction(SVM_prob[,2], tictac_test$win)
  
  nb_perf <- performance(nb_pred, measure = "tpr", x.measure = "fpr")
  plot(nb_perf, col = "blue", main = "Curvas ROC", xlab = "False Positive Rate (FPR)",
       ylab = "True positive Rate (TPR)", xlim = c(0, 1), ylim = c(0, 1), asp=1)
  plot(performance(nb_pred, measure = "tpr", x.measure = "fpr"), col = "red", add = TRUE)
  plot(performance(dt_pred, measure = "tpr", x.measure = "fpr"), col = "orange", add = TRUE)
  plot(performance(nn_pred, measure = "tpr", x.measure = "fpr"), col = "green", add = TRUE)
  plot(performance(knn_pred, measure = "tpr", x.measure = "fpr"), col = "black", add = TRUE)
  #plot(performance(SVM_pred, measure = "tpr", x.measure = "fpr"), col = "pink", add = TRUE)
  #legend("bottomright", legend = c("Naive Bayes", "SVM"), col = c("blue", "red"), lwd = 2)

```
