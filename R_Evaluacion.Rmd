---
output: 
  html_document:
    css: |
      pre, .r-output, .inline-output {
          margin-bottom: 30px;
      }
---

<!-- Configuración de celdas e input -->
```{r setup, include=FALSE}
# Global chunk options for all code cells
knitr::opts_chunk$set(
  root.dir = "C:/Users/andyg/Documents/MasterIA/2.CienciadeDatos/R_eval",    # Set the working directory for all chunks
  results = 'hold',    # Combine all outputs together in one block
  fig.show = 'hold',   # Show all plots together
  echo = TRUE          # Show the code along with the output (default)
)

cat("
<style>
pre, .r-output, .inline-output {
    margin-bottom: 30px;
}
</style>
")
```
<!-- Custom Header -->
---
<div style="margin-top: 50px;"></div>

<h1 style="text-align: center; font-size: 36px; font-family: Arial, sans-serif; color: #2c3e50;">
    <strong>Entrega B: R - Evaluación de Modelos</strong>
</h1>
<p style="text-align: center; font-size: 18px; font-family: Arial;">
    <strong>Andrés García-Serra Romero</strong> <br>
    <strong>Ciencia de Datos</strong>
</p>

<!-- Introduccion -->
---
<div style="margin-top: 20px;"></div>
<h1 style="font-size: 24px; font-family: Arial, sans-serif; color: #2c3e50;">
    <strong>Introducción</strong>
</h1>

En este trabajo aplicaremos los conocimientos aprendidos en evaluación de diferentes tipos de clasificadores de datos en la tarea de predicción de si un indivíduo ha ganado o no una partida de tres en raya, utilizando como datos de input la distribución del tablero de la partida. Para realizar la práctica hemos utilizado el software RStudio, creando un proyecto del tipo Markdown notebook. Utilizando el paquete *caret* de *R* crearemos cuatro modelos ...


<!-- 1. Importado datos -->
---
<h1 style="font-size: 24px; font-family: Arial, sans-serif; color: #2c3e50;">
    <strong>1.Importar Datos</strong>
</h1>

En primer lugar importaremos el fichero *tic-tac-toe.txt* con los datos en un dataframe, del que cambiaremos los nombres de las columnas para comprender mejor los datos, teniendo en cuenta de que se trata de un tablero 3x3 en el que las posiciones van desde arriba a la izquierda (*pos1*) hasta debajo a la derecha (*pos9*). Llamaremos *win* al último elemento de cada fila, que recoge la victoria o derrota del jugador "x".


```{r}
tictac <- data.frame(read.csv("tic-tac-toe.txt"))
header <- c("pos1","pos2","pos3","pos4","pos5","pos6","pos7","pos8","pos9","win")
colnames(tictac) <- header
head(tictac)
```

Como primer paso analizamos los datos en busca de valores vacíos, NaNs o valores que no sean los deseados. Estos son: "x", "o" o "b" entre los componentes de posición y "positive" o "negative" en el último componente de cada fila.

```{r}
any(is.na(tictac))
values <- c("x", "o", "b","positive", "negative")
any(!as.matrix(tictac[, 1:9]) %in% values[1:3])
any(!tictac[, 10] %in% values[4:5])

```
<!-- 2. Data Splitting -->

<h1 style="font-size: 24px; font-family: Arial, sans-serif; color: #2c3e50;">
    <strong>2. Data Splitting</strong>
</h1>

Dividimos el dataset en 70 training y 30 test. Comprobamos que efectivamente la proporción es 7/3.


```{r}
library(caret)
set.seed(100)
train_pos <- createDataPartition(tictac$win, p = .7, 
                                  list = FALSE, 
                                  times = 1)
tictac_train <- tictac[ train_pos,]
tictac_test  <- tictac[-train_pos,]
```

Comprobamos que la división de los datasets sigue la proporción 7/3 y también que la proporción de clases de ambos datasets es igual que la del oiginal.

```{r}
dim(tictac_train)[1]/dim(tictac_test)[1] - 7/3
sum(tictac[,10]=="positive")/sum(tictac[,10]=="negative")
sum(tictac_train[,10]=="positive")/sum(tictac_train[,10]=="negative")
sum(tictac_test[,10]=="positive")/sum(tictac_test[,10]=="negative")
```

<!-- 3. Generación de modelos -->
<h1 style="font-size: 24px; font-family: Arial, sans-serif; color: #2c3e50;">
    <strong>3. Generación de Modelos</strong>
</h1>

```{r}
library(caret)
library(klaR)
set.seed(100)
#Naive Bayes - nb
nb_fit <- train(win ~ ., data = tictac_train, method = "nb")
#Decision Tree - dt
#Neural Network - nn
#Nearest Neighbour - knn
#SVM (linear kernel) - svm

```

