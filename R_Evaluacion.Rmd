---
output: 
  html_document:
    css: styles.css
---

<!-- Configuración de celdas e input -->
```{r setup, include=FALSE}
# Global chunk options for all code cells
knitr::opts_chunk$set(
  root.dir = "C:/home/andy/Git/ia-model_eval/"    # Set the working directory for all chunks
  #results = 'hold',    # Combine all outputs together in one block
  #fig.show = 'hold',   # Show all plots together
  #echo = TRUE          # Show the code along with the output (default)
)
```
<!-- Custom Header -->
---
<div style="margin-top: 50px;"></div>

<h1 style="text-align: center; font-size: 36px; font-family: Arial, sans-serif; color: #2c3e50;">
    <strong>Entrega B: R - Evaluación de Modelos</strong>
</h1>
<p style="text-align: center; font-size: 18px; font-family: Arial;">
    <strong>Andrés García-Serra Romero</strong> <br>
    <strong>Ciencia de Datos</strong>
</p>

<!-- Introduccion -->
---
<div style="margin-top: 20px;"></div>
<h1 style="font-size: 24px; font-family: Arial, sans-serif; color: #2c3e50;">
    <strong>Introducción</strong>
</h1>

En este trabajo aplicaremos los conocimientos aprendidos en evaluación de diferentes tipos de clasificadores de datos en la tarea de predicción de si un indivíduo ha ganado o no una partida de tres en raya, utilizando como datos de input la distribución del tablero de la partida. Para realizar la práctica hemos utilizado el software RStudio, creando un proyecto del tipo Markdown notebook. Utilizando el paquete *caret* de *R* crearemos cuatro modelos ...


<!-- 1. Importando datos -->
---
<h1 style="font-size: 24px; font-family: Arial, sans-serif; color: #2c3e50;">
    <strong>1.Importar Datos</strong>
</h1>

En primer lugar importaremos el fichero *tic-tac-toe.txt* con los datos en un dataframe, del que cambiaremos los nombres de las columnas para comprender mejor los datos, teniendo en cuenta de que se trata de un tablero 3x3 en el que las posiciones van desde arriba a la izquierda (*pos1*) hasta debajo a la derecha (*pos9*). Llamaremos *win* al último elemento de cada fila, que recoge la victoria o derrota del jugador "x".
---
```{r}
tictac <- data.frame(read.csv("tic-tac-toe.txt"))
header <- c("pos1","pos2","pos3","pos4","pos5","pos6","pos7","pos8","pos9","win")
colnames(tictac) <- header
head(tictac)
```
---
Como primer paso analizamos los datos en busca de valores vacíos, NaNs o valores que no sean los deseados. Estos son: "x", "o" o "b" entre los componentes de posición y "positive" o "negative" en el último componente de cada fila.

```{r}
any(is.na(tictac))
values <- c("x", "o", "b","positive", "negative")
any(!as.matrix(tictac[, 1:9]) %in% values[1:3])
any(!tictac[, 10] %in% values[4:5])

```

<!-- 2. Data Splitting -->
---
<h1 style="font-size: 24px; font-family: Arial, sans-serif; color: #2c3e50;">
    <strong>2. Data Splitting</strong>
</h1>

Dividimos el dataset en 70 training y 30 test. Comprobamos que efectivamente la proporción es 7/3.

```{r, include=FALSE}
library(caret)
```

```{r}
set.seed(100)
train_pos <- createDataPartition(tictac$win, p = .7, 
                                  list = FALSE, 
                                  times = 1)
tictac_train <- tictac[ train_pos,]
tictac_test  <- tictac[-train_pos,]
```

Comprobamos que la división de los datasets sigue la proporción 7/3 y también que la proporción de clases de ambos datasets es igual que la del oiginal.

```{r}
dim(tictac_train)[1]/dim(tictac_test)[1] - 7/3
sum(tictac[,10]=="positive")/sum(tictac[,10]=="negative")
sum(tictac_train[,10]=="positive")/sum(tictac_train[,10]=="negative")
sum(tictac_test[,10]=="positive")/sum(tictac_test[,10]=="negative")
```

<!-- 3. Generación de modelos -->
---
<h1 style="font-size: 24px; font-family: Arial, sans-serif; color: #2c3e50;">
    <strong>3. Generación de Modelos</strong>
</h1>

```{r, include=FALSE}
library(caret)
library(klaR)
library(dplyr)
library(knitr)
library(kableExtra)
library(AUC)
library(kernlab)
library(ROCR)
```

```{r, results='hide'}
set.seed(100)

fitControl <- trainControl(
  method = "cv",                      # "cv" indica validación cruzada
  number = 10                         # Número de folds (5-fold cross-validation)
  #trace = FALSE                      # No queremos output de entrenamiento
  #classProbs = TRUE,                 # Habilitar predicción de probabilidades (para modelos soft)
  #summaryFunction = twoClassSummary  # Evaluar usando métricas como ROC y Kappa
)
set.seed(100)
models <- list(
  nb_train <- train(win ~ ., data = tictac_train, method = "nb", trControl = fitControl),         #Naive Bayes - nb
  dt_train <- train(win ~ ., data = tictac_train, method = "J48", trControl = fitControl),        #Decision Tree - dt
  nn_train <- train(win ~ ., data = tictac_train, method = "nnet", trControl = fitControl),       #Neural Network - nn
  knn_train <- train(win ~ ., data = tictac_train, method = "knn", trControl = fitControl)       #Nearest Neighbour - knn
  #SVM_train <- train(win ~ ., data = tictac_train, method = "svmLinear", trControl = fitControl)  #SVM (linear kernel) - svm
)

results <- bind_rows(lapply(models, function(model) {
  data.frame(
    Accuracy = max(model$results$Accuracy),  # Mejor accuracy
    Kappa = max(model$results$Kappa)         # Mejor kappa
  )
}))

Models <- c('Naive Bayes','Decision Tree','Neural Network','Nearest Neighbour')#,'SVM')
results <- cbind(Models, results)

```

```{r}
# Mostrar tabla con estilo en HTML
kable(results, format = "html") %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover"))
```

```{r}
nn_pred <- predict(nn_train,tictac_test)
nn_matrix <- confusionMatrix(nn_pred,factor(tictac_test$win))
nn_eval <- postResample(nn_pred,factor(tictac_test$win))
nn_eval[[1]]
```
---
Repitiendo esto para el resto de modelos, podemos extraer una tabla de accuracy y kappa de cada modelo aplicado a los datos de evaluación.

```{r}
set.seed(100)
eval <- list(
  nb_eval <- postResample(predict(nb_train,tictac_test),factor(tictac_test$win)),        #Naive Bayes - nb
  dt_eval <- postResample(predict(dt_train,tictac_test),factor(tictac_test$win)),     #Decision Tree - dt
  nn_eval <- postResample(predict(nn_train,tictac_test),factor(tictac_test$win)),     #Neural Network - nn
  knn_eval <- postResample(predict(knn_train,tictac_test),factor(tictac_test$win)) #Nearest Neighbour - knn
  #SVM_eval <- postResample(pred = predict(SVM_train,tictac_test),factor(tictac_test$win))               #SVM (linear kernel) - svm
)

eval_results <- bind_rows(lapply(eval, function(model) {
  data.frame(
    Accuracy = model[[1]],  # Mejor accuracy
    Kappa = model[[2]]      # Mejor kappa
  )
}))

eval_results <- cbind(Models, eval_results)

# Mostrar tabla con estilo en HTML
kable(eval_results, format = "html") %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover"))

```

```{r}
  set.seed(100)
  positives <- ifelse(tictac_test$win == "positive", 1, 0)
  nb_prob <- predict(nb_train,tictac_test, type = "prob")
  dt_prob <- predict(dt_train,tictac_test, type = "prob")
  nn_prob <- predict(nn_train,tictac_test, type = "prob")
  knn_prob <- predict(knn_train,tictac_test, type = "prob")
  
  nb_pred <- prediction(nb_prob[,2], tictac_test$win)
  dt_pred <- prediction(dt_prob[,2], tictac_test$win)
  nn_pred <- prediction(nn_prob[,2], tictac_test$win)
  knn_pred <- prediction(knn_prob[,2], tictac_test$win)
  
  nb_perf <- performance(nb_pred, measure = "tpr", x.measure = "fpr")
  plot(nb_perf, col = "blue", main = "Curvas ROC", xlab = "False Positive Rate (FPR)",
       ylab = "True positive Rate (TPR)", xlim = c(0, 1), ylim = c(0, 1), asp=1)
  plot(performance(dt_pred, measure = "tpr", x.measure = "fpr"), col = "red", add = TRUE)
  legend("bottomright", legend = c("Naive Bayes", "SVM"), col = c("blue", "red"), lwd = 2)

```


