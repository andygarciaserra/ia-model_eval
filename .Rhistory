sum(tictac_test[,10]=="positive")/sum(tictac_test[,10]=="negative")
set.seed(100)
fitControl <- trainControl(
method = "cv",                      # "cv" indica validación cruzada
number = 10,                         # Número de folds (5-fold cross-validation)
classProbs = TRUE,                 # Habilitar predicción de probabilidades (para modelos soft)
#summaryFunction = twoClassSummary  # Evaluar usando métricas como ROC y Kappa
)
set.seed(100)
models <- list(
nb_train <- train(win ~ ., data = tictac_train, method = "nb", trControl = fitControl),         #Naive Bayes - nb
dt_train <- train(win ~ ., data = tictac_train, method = "C5.0", trControl = fitControl),        #Decision Tree - dt
nn_train <- train(win ~ ., data = tictac_train, method = "nnet", trControl = fitControl),       #Neural Network - nn
knn_train <- train(win ~ ., data = tictac_train, method = "knn", trControl = fitControl)       #Nearest Neighbour - knn
#SVM_train <- train(win ~ ., data = tictac_train, method = "svmLinear", trControl = fitControl)  #SVM (linear kernel) - svm
)
results <- bind_rows(lapply(models, function(model) {
data.frame(
Accuracy = max(model$results$Accuracy),  # Mejor accuracy
Kappa = max(model$results$Kappa)         # Mejor kappa
)
}))
Models <- c('Naive Bayes','Decision Tree','Neural Network','Nearest Neighbour')#,'SVM')
results <- cbind(Models, results)
set.seed(100)
fitControl <- trainControl(
method = "cv",                      # "cv" indica validación cruzada
number = 10,                         # Número de folds (5-fold cross-validation)
classProbs = TRUE,                 # Habilitar predicción de probabilidades (para modelos soft)
)
set.seed(100)
models <- list(
nb_train <- train(win ~ ., data = tictac_train, method = "nb", trControl = fitControl),         #Naive Bayes - nb
dt_train <- train(win ~ ., data = tictac_train, method = "C5.0", trControl = fitControl),        #Decision Tree - dt
nn_train <- train(win ~ ., data = tictac_train, method = "nnet", trControl = fitControl),       #Neural Network - nn
knn_train <- train(win ~ ., data = tictac_train, method = "knn", trControl = fitControl),       #Nearest Neighbour - knn
SVM_train <- train(win ~ ., data = tictac_train, method = "svmLinear2", trControl = fitControl)  #SVM (linear kernel) - svm
)
results <- bind_rows(lapply(models, function(model) {
data.frame(
Accuracy = max(model$results$Accuracy),  # Mejor accuracy
Kappa = max(model$results$Kappa)         # Mejor kappa
)
}))
Models <- c('Naive Bayes','Decision Tree','Neural Network','Nearest Neighbour')#,'SVM')
results <- cbind(Models, results)
set.seed(100)
fitControl <- trainControl(
method = "cv",                      # "cv" indica validación cruzada
number = 10,                         # Número de folds (5-fold cross-validation)
classProbs = TRUE,                 # Habilitar predicción de probabilidades (para modelos soft)
)
set.seed(100)
models <- list(
nb_train <- train(win ~ ., data = tictac_train, method = "nb", trControl = fitControl),         #Naive Bayes - nb
dt_train <- train(win ~ ., data = tictac_train, method = "C5.0", trControl = fitControl),        #Decision Tree - dt
nn_train <- train(win ~ ., data = tictac_train, method = "nnet", trControl = fitControl),       #Neural Network - nn
knn_train <- train(win ~ ., data = tictac_train, method = "knn", trControl = fitControl),       #Nearest Neighbour - knn
SVM_train <- train(win ~ ., data = tictac_train, method = "svmLinear2", trControl = fitControl)  #SVM (linear kernel) - svm
)
results <- bind_rows(lapply(models, function(model) {
data.frame(
Accuracy = max(model$results$Accuracy),  # Mejor accuracy
Kappa = max(model$results$Kappa)         # Mejor kappa
)
}))
Models <- c('Naive Bayes','Decision Tree','Neural Network','Nearest Neighbour','SVM')
results <- cbind(Models, results)
# Mostrar tabla con estilo en HTML
kable(results, format = "html") %>%
kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover"))
nn_pred <- predict(nn_train,tictac_test)
nn_matrix <- confusionMatrix(nn_pred,factor(tictac_test$win))
nn_eval <- postResample(nn_pred,factor(tictac_test$win))
nn_eval[[1]]
set.seed(100)
eval <- list(
nb_eval <- postResample(predict(nb_train,tictac_test),factor(tictac_test$win)),        #Naive Bayes - nb
dt_eval <- postResample(predict(dt_train,tictac_test),factor(tictac_test$win)),     #Decision Tree - dt
nn_eval <- postResample(predict(nn_train,tictac_test),factor(tictac_test$win)),     #Neural Network - nn
knn_eval <- postResample(predict(knn_train,tictac_test),factor(tictac_test$win)) #Nearest Neighbour - knn
#SVM_eval <- postResample(pred = predict(SVM_train,tictac_test),factor(tictac_test$win))               #SVM (linear kernel) - svm
)
eval_results <- bind_rows(lapply(eval, function(model) {
data.frame(
Accuracy = model[[1]],  # Mejor accuracy
Kappa = model[[2]]      # Mejor kappa
)
}))
eval_results <- cbind(Models, eval_results)
set.seed(100)
eval <- list(
nb_eval <- postResample(predict(nb_train,tictac_test),factor(tictac_test$win)),        #Naive Bayes - nb
dt_eval <- postResample(predict(dt_train,tictac_test),factor(tictac_test$win)),     #Decision Tree - dt
nn_eval <- postResample(predict(nn_train,tictac_test),factor(tictac_test$win)),     #Neural Network - nn
knn_eval <- postResample(predict(knn_train,tictac_test),factor(tictac_test$win)), #Nearest Neighbour - knn
SVM_eval <- postResample(pred = predict(SVM_train,tictac_test),factor(tictac_test$win))               #SVM (linear kernel) - svm
)
eval_results <- bind_rows(lapply(eval, function(model) {
data.frame(
Accuracy = model[[1]],  # Mejor accuracy
Kappa = model[[2]]      # Mejor kappa
)
}))
eval_results <- cbind(Models, eval_results)
# Mostrar tabla con estilo en HTML
kable(eval_results, format = "html") %>%
kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover"))
set.seed(100)
positives <- ifelse(tictac_test$win == "positive", 1, 0)
nb_prob <- predict(nb_train,tictac_test, type = "prob")
dt_prob <- predict(dt_train,tictac_test, type = "prob")
nn_prob <- predict(nn_train,tictac_test, type = "prob")
knn_prob <- predict(knn_train,tictac_test, type = "prob")
nb_pred <- prediction(nb_prob[,2], tictac_test$win)
dt_pred <- prediction(dt_prob[,2], tictac_test$win)
nn_pred <- prediction(nn_prob[,2], tictac_test$win)
knn_pred <- prediction(knn_prob[,2], tictac_test$win)
nb_perf <- performance(nb_pred, measure = "tpr", x.measure = "fpr")
plot(nb_perf, col = "blue", main = "Curvas ROC", xlab = "False Positive Rate (FPR)",
ylab = "True positive Rate (TPR)", xlim = c(0, 1), ylim = c(0, 1), asp=1)
plot(performance(dt_pred, measure = "tpr", x.measure = "fpr"), col = "red", add = TRUE)
legend("bottomright", legend = c("Naive Bayes", "SVM"), col = c("blue", "red"), lwd = 2)
set.seed(100)
positives <- ifelse(tictac_test$win == "positive", 1, 0)
nb_prob <- predict(nb_train,tictac_test, type = "prob")
dt_prob <- predict(dt_train,tictac_test, type = "prob")
nn_prob <- predict(nn_train,tictac_test, type = "prob")
knn_prob <- predict(knn_train,tictac_test, type = "prob")
nb_pred <- prediction(nb_prob[,2], tictac_test$win)
dt_pred <- prediction(dt_prob[,2], tictac_test$win)
nn_pred <- prediction(nn_prob[,2], tictac_test$win)
knn_pred <- prediction(knn_prob[,2], tictac_test$win)
nb_perf <- performance(nb_pred, measure = "tpr", x.measure = "fpr")
plot(nb_perf, col = "blue", main = "Curvas ROC", xlab = "False Positive Rate (FPR)",
ylab = "True positive Rate (TPR)", xlim = c(0, 1), ylim = c(0, 1), asp=1)
plot(performance(dt_pred, measure = "tpr", x.measure = "fpr"), col = "red", add = TRUE)
plot(performance(nn_pred, measure = "tpr", x.measure = "fpr"), col = "orange", add = TRUE)
plot(performance(knn_pred, measure = "tpr", x.measure = "fpr"), col = "green", add = TRUE)
plot(performance(SVM_pred, measure = "tpr", x.measure = "fpr"), col = "black", add = TRUE)
set.seed(100)
eval <- list(
nb_eval <- postResample(predict(nb_train,tictac_test),factor(tictac_test$win)),     #Naive Bayes - nb
dt_eval <- postResample(predict(dt_train,tictac_test),factor(tictac_test$win)),     #Decision Tree - dt
nn_eval <- postResample(predict(nn_train,tictac_test),factor(tictac_test$win)),     #Neural Network - nn
knn_eval <- postResample(predict(knn_train,tictac_test),factor(tictac_test$win)),   #Nearest Neighbour - knn
knn_eval <- postResample(predict(SVM_train,tictac_test),factor(tictac_test$win))    #SVM (linear kernel) - svm
)
eval_results <- bind_rows(lapply(eval, function(model) {
data.frame(
Accuracy = model[[1]],  # Mejor accuracy
Kappa = model[[2]]      # Mejor kappa
)
}))
eval_results <- cbind(Models, eval_results)
# Mostrar tabla con estilo en HTML
kable(eval_results, format = "html") %>%
kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover"))
set.seed(100)
positives <- ifelse(tictac_test$win == "positive", 1, 0)
nb_prob <- predict(nb_train,tictac_test, type = "prob")
dt_prob <- predict(dt_train,tictac_test, type = "prob")
nn_prob <- predict(nn_train,tictac_test, type = "prob")
knn_prob <- predict(knn_train,tictac_test, type = "prob")
nb_pred <- prediction(nb_prob[,2], tictac_test$win)
dt_pred <- prediction(dt_prob[,2], tictac_test$win)
nn_pred <- prediction(nn_prob[,2], tictac_test$win)
knn_pred <- prediction(knn_prob[,2], tictac_test$win)
nb_perf <- performance(nb_pred, measure = "tpr", x.measure = "fpr")
plot(nb_perf, col = "blue", main = "Curvas ROC", xlab = "False Positive Rate (FPR)",
ylab = "True positive Rate (TPR)", xlim = c(0, 1), ylim = c(0, 1), asp=1)
plot(performance(dt_pred, measure = "tpr", x.measure = "fpr"), col = "red", add = TRUE)
plot(performance(nn_pred, measure = "tpr", x.measure = "fpr"), col = "orange", add = TRUE)
plot(performance(knn_pred, measure = "tpr", x.measure = "fpr"), col = "green", add = TRUE)
plot(performance(SVM_pred, measure = "tpr", x.measure = "fpr"), col = "black", add = TRUE)
set.seed(100)
train_pos <- createDataPartition(tictac$win, p = .7,
list = FALSE,
times = 1)
tictac_train <- tictac[ train_pos,]
tictac_test  <- tictac[-train_pos,]
tictac <- data.frame(read.csv("tic-tac-toe.txt"))
# Global chunk options for all code cells
knitr::opts_chunk$set(
root.dir = "/home/andy/Git/ia-model_eval/",
results = 'hold'
)
library(caret)
library(klaR)
library(dplyr)
library(knitr)
library(kableExtra)
library(AUC)
library(kernlab)
library(e1071)
library(pROC)
library(ROCR)
library(plyr)
library(C50)
tictac <- data.frame(read.csv("tic-tac-toe.txt"))
set.wd(/home/andy/Git/ia-model_eval/)
set.wd(/home/andy/Git/ia-model_eval)
set.wd(~/Git/ia-model_eval/)
set.wd(/home/andy/Git/ia-model_eval)
setwd("~/Git/ia-model_eval")
tictac <- data.frame(read.csv("tic-tac-toe.txt"))
setwd("~/Git/ia-model_eval")
tictac <- data.frame(read.csv("tic-tac-toe.txt"))
tictac <- data.frame(read.csv("tic-tac-toe.txt"))
header <- c("pos1","pos2","pos3","pos4","pos5","pos6","pos7","pos8","pos9","win")
colnames(tictac) <- header
head(tictac)
any(!as.matrix(tictac[, 1:9]) %in% values[1:3])"~/Git/ia-model_eval/tic-tac-toe.txt"
any(is.na(tictac))
values <- c("x", "o", "b","positive", "negative")
any(!as.matrix(tictac[, 1:9]) %in% values[1:3])
any(!tictac[, 10] %in% values[4:5])
set.seed(100)
train_pos <- createDataPartition(tictac$win, p = .7,
list = FALSE,
times = 1)
tictac_train <- tictac[ train_pos,]
tictac_test  <- tictac[-train_pos,]
dim(tictac_train)[1]/dim(tictac_test)[1] - 7/3
sum(tictac[,10]=="positive")/sum(tictac[,10]=="negative")
sum(tictac_train[,10]=="positive")/sum(tictac_train[,10]=="negative")
sum(tictac_test[,10]=="positive")/sum(tictac_test[,10]=="negative")
# Ratio entre el Set de Entrenamiento y el de Evaluación respecto a 7/3
test_train_ratio <- data.frame(
Name = c("Sets Size Ratio to 7/3"),
value = dim(tictac_train)[1]/dim(tictac_test)[1] / (7/3)
)
kable(test_train_ratio, format = "html") %>%
kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover"))
# Ratios de Positivos/Negativos para cada set
sets <- data.frame(
Set = c("Pos/Neg Ratio"),
Full_Dataset = c(sum(tictac[,10]=="positive")/sum(tictac[,10]=="negative")),
Training_Set = c(sum(tictac_train[,10]=="positive")/sum(tictac_train[,10]=="negative")),
Validation_Set = c(sum(tictac_test[,10]=="positive")/sum(tictac_test[,10]=="negative"))
)
kable(sets, col.names = c("", "Full Dataset", "Training Set", "Validation Set"), format = "html") %>%
kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover"))
set.seed(100)
fitControl <- trainControl(
method = "cv",                      # "cv" indica validación cruzada
number = 10,                         # Número de folds (5-fold cross-validation)
classProbs = TRUE,                 # Habilitar predicción de probabilidades (para modelos soft)
)
set.seed(100)
models <- list(
nb_train <- train(win ~ ., data = tictac_train, method = "nb", trControl = fitControl),         #Naive Bayes - nb
dt_train <- train(win ~ ., data = tictac_train, method = "C5.0", trControl = fitControl),        #Decision Tree - dt
nn_train <- train(win ~ ., data = tictac_train, method = "nnet", trControl = fitControl),       #Neural Network - nn
knn_train <- train(win ~ ., data = tictac_train, method = "knn", trControl = fitControl),       #Nearest Neighbour - knn
SVM_train <- train(win ~ ., data = tictac_train, method = "svmLinear2", trControl = fitControl)  #SVM (linear kernel) - svm
)
results <- bind_rows(lapply(models, function(model) {
data.frame(
Accuracy = max(model$results$Accuracy),  # Mejor accuracy
Kappa = max(model$results$Kappa)         # Mejor kappa
)
}))
Models <- c('Naive Bayes','Decision Tree','Neural Network','Nearest Neighbour','SVM')
results <- cbind(Models, results)
# Mostrar tabla con estilo en HTML
kable(results, format = "html") %>%
kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover"))
nn_pred <- predict(nn_train,tictac_test)
nn_matrix <- confusionMatrix(nn_pred,factor(tictac_test$win))
nn_eval <- postResample(nn_pred,factor(tictac_test$win))
nn_eval[[1]]
set.seed(100)
eval <- list(
nb_eval <- postResample(predict(nb_train,tictac_test),factor(tictac_test$win)),     #Naive Bayes - nb
dt_eval <- postResample(predict(dt_train,tictac_test),factor(tictac_test$win)),     #Decision Tree - dt
nn_eval <- postResample(predict(nn_train,tictac_test),factor(tictac_test$win)),     #Neural Network - nn
knn_eval <- postResample(predict(knn_train,tictac_test),factor(tictac_test$win)),   #Nearest Neighbour - knn
knn_eval <- postResample(predict(SVM_train,tictac_test),factor(tictac_test$win))    #SVM (linear kernel) - svm
)
eval_results <- bind_rows(lapply(eval, function(model) {
data.frame(
Accuracy = model[[1]],  # Mejor accuracy
Kappa = model[[2]]      # Mejor kappa
)
}))
eval_results <- cbind(Models, eval_results)
# Mostrar tabla con estilo en HTML
kable(eval_results, format = "html") %>%
kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover"))
set.seed(100)
positives <- ifelse(tictac_test$win == "positive", 1, 0)
nb_prob <- predict(nb_train,tictac_test, type = "prob")
dt_prob <- predict(dt_train,tictac_test, type = "prob")
nn_prob <- predict(nn_train,tictac_test, type = "prob")
knn_prob <- predict(knn_train,tictac_test, type = "prob")
nb_pred <- prediction(nb_prob[,2], tictac_test$win)
dt_pred <- prediction(dt_prob[,2], tictac_test$win)
nn_pred <- prediction(nn_prob[,2], tictac_test$win)
knn_pred <- prediction(knn_prob[,2], tictac_test$win)
nb_perf <- performance(nb_pred, measure = "tpr", x.measure = "fpr")
plot(nb_perf, col = "blue", main = "Curvas ROC", xlab = "False Positive Rate (FPR)",
ylab = "True positive Rate (TPR)", xlim = c(0, 1), ylim = c(0, 1), asp=1)
plot(performance(dt_pred, measure = "tpr", x.measure = "fpr"), col = "red", add = TRUE)
plot(performance(nn_pred, measure = "tpr", x.measure = "fpr"), col = "orange", add = TRUE)
plot(performance(knn_pred, measure = "tpr", x.measure = "fpr"), col = "green", add = TRUE)
plot(performance(SVM_pred, measure = "tpr", x.measure = "fpr"), col = "black", add = TRUE)
set.seed(100)
positives <- ifelse(tictac_test$win == "positive", 1, 0)
nb_prob <- predict(nb_train,tictac_test, type = "prob")
dt_prob <- predict(dt_train,tictac_test, type = "prob")
nn_prob <- predict(nn_train,tictac_test, type = "prob")
knn_prob <- predict(knn_train,tictac_test, type = "prob")
SVM_prob <- predict(SVM_train,tictac_test, type = "prob")
nb_pred <- prediction(nb_prob[,2], tictac_test$win)
dt_pred <- prediction(dt_prob[,2], tictac_test$win)
nn_pred <- prediction(nn_prob[,2], tictac_test$win)
knn_pred <- prediction(knn_prob[,2], tictac_test$win)
SVM_pred <- prediction(SVM_prob[,2], tictac_test$win)
nb_perf <- performance(nb_pred, measure = "tpr", x.measure = "fpr")
plot(nb_perf, col = "blue", main = "Curvas ROC", xlab = "False Positive Rate (FPR)",
ylab = "True positive Rate (TPR)", xlim = c(0, 1), ylim = c(0, 1), asp=1)
plot(performance(dt_pred, measure = "tpr", x.measure = "fpr"), col = "red", add = TRUE)
plot(performance(nn_pred, measure = "tpr", x.measure = "fpr"), col = "orange", add = TRUE)
plot(performance(knn_pred, measure = "tpr", x.measure = "fpr"), col = "green", add = TRUE)
plot(performance(SVM_pred, measure = "tpr", x.measure = "fpr"), col = "black", add = TRUE)
legend("bottomright", legend = c("Naive Bayes", "SVM"), col = c("blue", "red"), lwd = 2)
set.seed(100)
positives <- ifelse(tictac_test$win == "positive", 1, 0)
nb_prob <- predict(nb_train,tictac_test, type = "prob")
dt_prob <- predict(dt_train,tictac_test, type = "prob")
nn_prob <- predict(nn_train,tictac_test, type = "prob")
knn_prob <- predict(knn_train,tictac_test, type = "prob")
SVM_prob <- predict(SVM_train,tictac_test, type = "prob")
nb_pred <- prediction(nb_prob[,2], tictac_test$win)
dt_pred <- prediction(dt_prob[,2], tictac_test$win)
nn_pred <- prediction(nn_prob[,2], tictac_test$win)
knn_pred <- prediction(knn_prob[,2], tictac_test$win)
SVM_pred <- prediction(SVM_prob[,2], tictac_test$win)
nb_perf <- performance(nb_pred, measure = "tpr", x.measure = "fpr")
plot(nb_perf, col = "blue", main = "Curvas ROC", xlab = "False Positive Rate (FPR)",
ylab = "True positive Rate (TPR)", xlim = c(0, 1), ylim = c(0, 1), asp=1)
plot(performance(dt_pred, measure = "tpr", x.measure = "fpr"), col = "red", add = TRUE)
plot(performance(nn_pred, measure = "tpr", x.measure = "fpr"), col = "orange", add = TRUE)
plot(performance(knn_pred, measure = "tpr", x.measure = "fpr"), col = "green", add = TRUE)
plot(performance(SVM_pred, measure = "tpr", x.measure = "fpr"), col = "black", add = TRUE)
legend("bottomright", legend = Models, col = c("blue", "red","orange","green","black"), lwd = 2)
set.seed(100)
fitControl <- trainControl(
method = "cv",                      # "cv" indica validación cruzada
number = 10,                         # Número de folds (5-fold cross-validation)
classProbs = TRUE,                 # Habilitar predicción de probabilidades (para modelos soft)
trace = FALSE
)
set.seed(100)
fitControl <- trainControl(
method = "cv",                      # "cv" indica validación cruzada
number = 10,                         # Número de folds (5-fold cross-validation)
classProbs = TRUE                 # Habilitar predicción de probabilidades (para modelos soft)
)
set.seed(100)
models <- list(
nb_train <- train(win ~ ., data = tictac_train, method = "nb", trControl = fitControl, trace = FALSE),         #Naive Bayes - nb
dt_train <- train(win ~ ., data = tictac_train, method = "C5.0", trControl = fitControl),        #Decision Tree - dt
nn_train <- train(win ~ ., data = tictac_train, method = "nnet", trControl = fitControl),       #Neural Network - nn
knn_train <- train(win ~ ., data = tictac_train, method = "knn", trControl = fitControl),       #Nearest Neighbour - knn
SVM_train <- train(win ~ ., data = tictac_train, method = "svmLinear2", trControl = fitControl)  #SVM (linear kernel) - svm
)
results <- bind_rows(lapply(models, function(model) {
data.frame(
Accuracy = max(model$results$Accuracy),  # Mejor accuracy
Kappa = max(model$results$Kappa)         # Mejor kappa
)
}))
Models <- c('Naive Bayes','Decision Tree','Neural Network','Nearest Neighbour','SVM')
results <- cbind(Models, results)
set.seed(100)
fitControl <- trainControl(
method = "cv",                      # "cv" indica validación cruzada
number = 10,                         # Número de folds (5-fold cross-validation)
classProbs = TRUE                 # Habilitar predicción de probabilidades (para modelos soft)
)
set.seed(100)
models <- list(
nb_train <- train(win ~ ., data = tictac_train, method = "nb", trControl = fitControl),         #Naive Bayes - nb
dt_train <- train(win ~ ., data = tictac_train, method = "C5.0", trControl = fitControl),        #Decision Tree - dt
nn_train <- train(win ~ ., data = tictac_train, method = "nnet", trControl = fitControl),       #Neural Network - nn
knn_train <- train(win ~ ., data = tictac_train, method = "knn", trControl = fitControl),       #Nearest Neighbour - knn
SVM_train <- train(win ~ ., data = tictac_train, method = "svmLinear2", trControl = fitControl)  #SVM (linear kernel) - svm
)
results <- bind_rows(lapply(models, function(model) {
data.frame(
Accuracy = max(model$results$Accuracy),  # Mejor accuracy
Kappa = max(model$results$Kappa)         # Mejor kappa
)
}))
Models <- c('Naive Bayes','Decision Tree','Neural Network','Nearest Neighbour','SVM')
results <- cbind(Models, results)
# Global chunk options for all code cells
knitr::opts_chunk$set(
root.dir = "/home/andy/Git/ia-model_eval/",
results = 'hold'
)
library(caret)
library(klaR)
library(dplyr)
library(knitr)
library(kableExtra)
library(AUC)
library(kernlab)
library(e1071)
library(pROC)
library(ROCR)
library(plyr)
library(C50)
tictac <- data.frame(read.csv("tic-tac-toe.txt"))
header <- c("pos1","pos2","pos3","pos4","pos5","pos6","pos7","pos8","pos9","win")
colnames(tictac) <- header
head(tictac)
any(is.na(tictac))
values <- c("x", "o", "b","positive", "negative")
any(!as.matrix(tictac[, 1:9]) %in% values[1:3])
any(!tictac[, 10] %in% values[4:5])
set.seed(100)
train_pos <- createDataPartition(tictac$win, p = .7,
list = FALSE,
times = 1)
tictac_train <- tictac[ train_pos,]
tictac_test  <- tictac[-train_pos,]
# Ratio entre el Set de Entrenamiento y el de Evaluación respecto a 7/3
test_train_ratio <- data.frame(
Name = c("Sets Size Ratio to 7/3"),
value = dim(tictac_train)[1]/dim(tictac_test)[1] / (7/3)
)
kable(test_train_ratio, format = "html") %>%
kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover"))
# Ratios de Positivos/Negativos para cada set
sets <- data.frame(
Set = c("Pos/Neg Ratio"),
Full_Dataset = c(sum(tictac[,10]=="positive")/sum(tictac[,10]=="negative")),
Training_Set = c(sum(tictac_train[,10]=="positive")/sum(tictac_train[,10]=="negative")),
Validation_Set = c(sum(tictac_test[,10]=="positive")/sum(tictac_test[,10]=="negative"))
)
kable(sets, col.names = c("", "Full Dataset", "Training Set", "Validation Set"), format = "html") %>%
kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover"))
set.seed(100)
fitControl <- trainControl(
method = "cv",                      # "cv" indica validación cruzada
number = 10,                         # Número de folds (5-fold cross-validation)
classProbs = TRUE                 # Habilitar predicción de probabilidades (para modelos soft)
)
set.seed(100)
models <- list(
nb_train <- train(win ~ ., data = tictac_train, method = "nb", trControl = fitControl),         #Naive Bayes - nb
dt_train <- train(win ~ ., data = tictac_train, method = "C5.0", trControl = fitControl),        #Decision Tree - dt
nn_train <- train(win ~ ., data = tictac_train, method = "nnet", trControl = fitControl),       #Neural Network - nn
knn_train <- train(win ~ ., data = tictac_train, method = "knn", trControl = fitControl),       #Nearest Neighbour - knn
SVM_train <- train(win ~ ., data = tictac_train, method = "svmLinear2", trControl = fitControl)  #SVM (linear kernel) - svm
)
results <- bind_rows(lapply(models, function(model) {
data.frame(
Accuracy = max(model$results$Accuracy),  # Mejor accuracy
Kappa = max(model$results$Kappa)         # Mejor kappa
)
}))
Models <- c('Naive Bayes','Decision Tree','Neural Network','Nearest Neighbour','SVM')
results <- cbind(Models, results)
# Mostrar tabla con estilo en HTML
kable(results, format = "html") %>%
kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover"))
nn_pred <- predict(nn_train,tictac_test)
nn_matrix <- confusionMatrix(nn_pred,factor(tictac_test$win))
nn_eval <- postResample(nn_pred,factor(tictac_test$win))
nn_eval[[1]]
set.seed(100)
eval <- list(
nb_eval <- postResample(predict(nb_train,tictac_test),factor(tictac_test$win)),     #Naive Bayes - nb
dt_eval <- postResample(predict(dt_train,tictac_test),factor(tictac_test$win)),     #Decision Tree - dt
nn_eval <- postResample(predict(nn_train,tictac_test),factor(tictac_test$win)),     #Neural Network - nn
knn_eval <- postResample(predict(knn_train,tictac_test),factor(tictac_test$win)),   #Nearest Neighbour - knn
knn_eval <- postResample(predict(SVM_train,tictac_test),factor(tictac_test$win))    #SVM (linear kernel) - svm
)
eval_results <- bind_rows(lapply(eval, function(model) {
data.frame(
Accuracy = model[[1]],  # Mejor accuracy
Kappa = model[[2]]      # Mejor kappa
)
}))
eval_results <- cbind(Models, eval_results)
# Mostrar tabla con estilo en HTML
kable(eval_results, format = "html") %>%
kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover"))
set.seed(100)
positives <- ifelse(tictac_test$win == "positive", 1, 0)
nb_prob <- predict(nb_train,tictac_test, type = "prob")
dt_prob <- predict(dt_train,tictac_test, type = "prob")
nn_prob <- predict(nn_train,tictac_test, type = "prob")
knn_prob <- predict(knn_train,tictac_test, type = "prob")
SVM_prob <- predict(SVM_train,tictac_test, type = "prob")
nb_pred <- prediction(nb_prob[,2], tictac_test$win)
dt_pred <- prediction(dt_prob[,2], tictac_test$win)
nn_pred <- prediction(nn_prob[,2], tictac_test$win)
knn_pred <- prediction(knn_prob[,2], tictac_test$win)
SVM_pred <- prediction(SVM_prob[,2], tictac_test$win)
nb_perf <- performance(nb_pred, measure = "tpr", x.measure = "fpr")
plot(nb_perf, col = "blue", main = "Curvas ROC", xlab = "False Positive Rate (FPR)",
ylab = "True positive Rate (TPR)", xlim = c(0, 1), ylim = c(0, 1), asp=1)
plot(performance(dt_pred, measure = "tpr", x.measure = "fpr"), col = "red", add = TRUE)
plot(performance(nn_pred, measure = "tpr", x.measure = "fpr"), col = "orange", add = TRUE)
plot(performance(knn_pred, measure = "tpr", x.measure = "fpr"), col = "green", add = TRUE)
plot(performance(SVM_pred, measure = "tpr", x.measure = "fpr"), col = "black", add = TRUE)
legend("bottomright", legend = Models, col = c("blue", "red","orange","green","black"), lwd = 2)
